{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "z8o3HBAmspak"
      },
      "outputs": [],
      "source": [
        "#!wget https://dataworks.indianapolis.iu.edu/bitstream/handle/11243/41/data.zip\n",
        "#!unzip -q data.zip\n",
        "#!rm data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTwIpTTYsaIR",
        "outputId": "1d33d159-8db0-4c0a-c042-311c07b15310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pc1BZG2saIW"
      },
      "source": [
        "# Dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "n4jHfAYRsaIY"
      },
      "outputs": [],
      "source": [
        "class ImageDNATrainDataset():\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "\n",
        "        TRAINING_SAMPLES_NUMBER = 12481\n",
        "        TRAINING_LABELS_NUMBER = 652\n",
        "\n",
        "        assert len(train_loc[0]) == TRAINING_SAMPLES_NUMBER\n",
        "\n",
        "        indeces = train_loc\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(\n",
        "            data_mat[\"embeddings_img\"][indeces]\n",
        "        ).float()\n",
        "        self.embeddings_dna = torch.from_numpy(\n",
        "            data_mat[\"embeddings_dna\"][indeces]\n",
        "        ).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        species_mapping = seen_species_mapping\n",
        "        assert len(species_mapping) == TRAINING_LABELS_NUMBER\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_species = np.array([species_mapping[label.item()] for label in species])\n",
        "        self.remapped_species = torch.from_numpy(remapped_species).long()\n",
        "\n",
        "        assert len(torch.unique(self.remapped_species)) == TRAINING_LABELS_NUMBER\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        assert len(self.genera) == TRAINING_SAMPLES_NUMBER\n",
        "\n",
        "        self.species_names = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_species[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation set\n",
        "- Number of samples: 6939.\n",
        "- Number of seen species of the training set in the validation set: 629.\n",
        "- Number of unseen species in the validation set: 97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GasJN0zyusZi"
      },
      "outputs": [],
      "source": [
        "class ImageDNAValidationDataset():\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_LABELS_NUMBER = 652\n",
        "        VALIDATION_SAMPLES_NUMBER = 6939\n",
        "        VALIDATION_SPECIES_NUMBER = 774\n",
        "        TRAINING_VALIDATION_SPECIES_NUMBER = 797\n",
        "        VALIDATION_SEEN_SPECIES_NUMBER = 629\n",
        "        VALIDATION_UNSEEN_SPECIES_GENERA_NUMBER = 97\n",
        "\n",
        "        indeces = np.concatenate((val_seen_loc, val_unseen_loc), axis=1)\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "        assert len(indeces) == VALIDATION_SAMPLES_NUMBER\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_mapping = {label: i + TRAINING_LABELS_NUMBER for i, label in enumerate(np.unique(unseen_species))}\n",
        "\n",
        "        # Union of the two mappings, allows to fully remap all the labels\n",
        "        species_mapping = seen_species_mapping | unseen_species_mapping\n",
        "        assert len(species_mapping) == TRAINING_VALIDATION_SPECIES_NUMBER\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_species = np.array([species_mapping[label.item()] for label in species])\n",
        "        self.remapped_species = torch.from_numpy(remapped_species).long()\n",
        "        assert len(torch.unique(self.remapped_species)) == VALIDATION_SPECIES_NUMBER\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        # Compute genera of unseen species in the validation set\n",
        "        unseen_species_genera = []\n",
        "        for i in val_unseen_loc[0]:\n",
        "            unseen_species_genera.append(data_mat[\"G\"][data_mat[\"labels\"][i][0] - 1][0] - 1041)\n",
        "        self.unseen_species_genera = np.array(unseen_species_genera)\n",
        "        assert len(np.unique(self.unseen_species_genera)) == VALIDATION_UNSEEN_SPECIES_GENERA_NUMBER\n",
        "\n",
        "        # Compute seen species number in the validation set\n",
        "        seen_species = []\n",
        "        for i in val_seen_loc[0]:\n",
        "            seen_species.append(species_mapping[data_mat[\"labels\"][i].item()])\n",
        "        self.seen_species = np.array(seen_species)\n",
        "        assert len(np.unique(self.seen_species)) == VALIDATION_SEEN_SPECIES_NUMBER\n",
        "\n",
        "        self.species_names = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_species[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test set\n",
        "- Number of samples: 13428.\n",
        "- Number of seen species of the training and validation set in the test set: 770.\n",
        "- Number of unseen species in the test set: 134"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "MKdIHeWZusZj"
      },
      "outputs": [],
      "source": [
        "class ImageDNATestDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        trainval_loc = splits_mat[\"trainval_loc\"]-1\n",
        "        test_seen_loc = splits_mat[\"test_seen_loc\"]-1\n",
        "        test_unseen_loc = splits_mat[\"test_unseen_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_SPECIES_NUMBER = 652\n",
        "        TRAINING_VALIDATION_SPECIES = 797\n",
        "        NUMBER_OF_SPECIES = 1040\n",
        "        TEST_SEEN_SPECIES_NUMBER = 770\n",
        "        TEST_UNSEEN_SPECIES_GENERA_NUMBER = 134\n",
        "\n",
        "        indeces = np.concatenate((test_seen_loc, test_unseen_loc), axis=1)\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species_validation = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_validation_mapping = {label: i + TRAINING_SPECIES_NUMBER for i, label in enumerate(np.unique(unseen_species_validation))}\n",
        "\n",
        "        # Remap unseen species during test in [797, 1039]\n",
        "        unseen_species_test = data_mat[\"labels\"][test_unseen_loc][0]\n",
        "        unseen_species_test_mapping = {label: i + TRAINING_VALIDATION_SPECIES for i, label in enumerate(np.unique(unseen_species_test))}\n",
        "\n",
        "        assert reduce(np.intersect1d, (seen_species, unseen_species_validation, unseen_species_test)).size == 0\n",
        "\n",
        "        # Union of the two mappings, allows to full remap all the labels\n",
        "        labels_mapping = seen_species_mapping | unseen_species_validation_mapping | unseen_species_test_mapping\n",
        "        assert len(labels_mapping) == NUMBER_OF_SPECIES\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_labels = np.array([labels_mapping[label.item()] for label in species])\n",
        "        self.remapped_labels = torch.from_numpy(remapped_labels).long()\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        # Compute genera of unseen species\n",
        "        unseen_species_genera = []\n",
        "        for i in test_unseen_loc[0]:\n",
        "            unseen_species_genera.append(data_mat[\"G\"][data_mat[\"labels\"][i][0] - 1][0] - 1041)\n",
        "\n",
        "        self.unseen_species_genera = np.array(unseen_species_genera)\n",
        "        assert len(np.unique(self.unseen_species_genera)) == TEST_UNSEEN_SPECIES_GENERA_NUMBER\n",
        "\n",
        "        # Compute seen species\n",
        "        seen_species = []\n",
        "        for i in test_seen_loc[0]:\n",
        "            seen_species.append(labels_mapping[data_mat[\"labels\"][i].item()])\n",
        "        self.seen_species = np.array(seen_species)\n",
        "        assert len(np.unique(self.seen_species)) == TEST_SEEN_SPECIES_NUMBER\n",
        "\n",
        "        self.species_name = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_labels[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "PWLG3L3eusZl"
      },
      "outputs": [],
      "source": [
        "class ImageDNATrainValidationDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        trainval_loc = splits_mat[\"trainval_loc\"]-1\n",
        "        test_seen_loc = splits_mat[\"test_seen_loc\"]-1\n",
        "        test_unseen_loc = splits_mat[\"test_unseen_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_SPECIES_NUMBER = 652\n",
        "        TRAINING_VALIDATION_SPECIES = 797\n",
        "        NUMBER_OF_SPECIES = 1040\n",
        "\n",
        "        indeces = trainval_loc\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species_validation = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_validation_mapping = {label: i + TRAINING_SPECIES_NUMBER for i, label in enumerate(np.unique(unseen_species_validation))}\n",
        "\n",
        "        # Remap unseen species during test in [797, 1039]\n",
        "        unseen_species_test = data_mat[\"labels\"][test_unseen_loc][0]\n",
        "        unseen_species_test_mapping = {label: i + TRAINING_VALIDATION_SPECIES for i, label in enumerate(np.unique(unseen_species_test))}\n",
        "\n",
        "        assert reduce(np.intersect1d, (seen_species, unseen_species_validation, unseen_species_test)).size == 0\n",
        "\n",
        "        # Union of the two mappings, allows to full remap all the labels\n",
        "        labels_mapping = seen_species_mapping | unseen_species_validation_mapping | unseen_species_test_mapping\n",
        "        assert len(labels_mapping) == NUMBER_OF_SPECIES\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]  # Consider only train\n",
        "        remapped_labels = np.array([labels_mapping[label.item()] for label in species])\n",
        "        self.remapped_labels = torch.from_numpy(remapped_labels).long()\n",
        "\n",
        "        assert len(torch.unique(self.remapped_labels)) == TRAINING_VALIDATION_SPECIES\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        self.species = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_labels[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg3l8IA0saIb"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fp_osQc7usZm"
      },
      "outputs": [],
      "source": [
        "class AttentionNet(nn.Module):\n",
        "        def __init__(self, num_seen_species, num_genera, embed_dim_1, num_heads_1, embed_dim_2, num_heads_2, embed_dim_3, num_heads_3):\n",
        "                super(AttentionNet, self).__init__()\n",
        "\n",
        "                self.img_fc1 = nn.Linear(2048, 1024)\n",
        "                self.img_fc2 = nn.Linear(1024, 500)\n",
        "\n",
        "                self.inception_encoders_block_1 = InceptionEncodersBlock(embed_dim_1, num_heads_1, embed_dim_2, num_heads_2, embed_dim_3, num_heads_3)\n",
        "\n",
        "                self.fc_species_1 = nn.Linear(4000, 1000)\n",
        "                self.fc_species_2 = nn.Linear(1000, num_seen_species)\n",
        "\n",
        "                self.fc_genera_1 = nn.Linear(4000, 500)\n",
        "                self.fc_genera_2 = nn.Linear(500, num_genera)\n",
        "\n",
        "                self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "                x_img = F.relu(self.img_fc1(x_img))\n",
        "                x_img = self.img_fc2(x_img)\n",
        "\n",
        "                x_img, x_dna = self.inception_encoders_block_1(x_img, x_dna)\n",
        "\n",
        "                x = torch.cat((x_img, x_dna), axis=1)\n",
        "\n",
        "                x_species = x.clone()\n",
        "                x_genera = x.clone()\n",
        "\n",
        "                x_species = self.dropout(F.relu(self.fc_species_1(x_species)))\n",
        "                x_species = self.fc_species_2(x_species)\n",
        "\n",
        "                x_genera = self.dropout(F.relu(self.fc_genera_1(x_genera)))\n",
        "                x_genera = self.fc_genera_2(x_genera)\n",
        "\n",
        "                return x_species, x_genera\n",
        "\n",
        "class ImageDNAEncoder(nn.Module):\n",
        "        def __init__(self, embed_dim, linear_dim, num_heads):\n",
        "                super(ImageDNAEncoder, self).__init__()\n",
        "                self.multi_head_img_1 = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "                self.multi_head_dna_1 = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "                self.norm_img_1 = nn.LayerNorm(embed_dim)\n",
        "                self.norm_dna_1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "                self.linear_img_1 = nn.Linear(embed_dim, linear_dim)\n",
        "                self.dropout_img = nn.Dropout(0.5)\n",
        "                self.linear_img_2 = nn.Linear(linear_dim, embed_dim)\n",
        "\n",
        "                self.linear_dna_1 = nn.Linear(embed_dim, linear_dim)\n",
        "                self.dropout_dna = nn.Dropout(0.5)\n",
        "                self.linear_dna_2 = nn.Linear(linear_dim, embed_dim)\n",
        "\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "                identity = x_img\n",
        "                x_img, _ = self.multi_head_img_1(x_img, x_dna, x_dna)\n",
        "                x_img = self.norm_img_1(x_img + identity)\n",
        "                x_img = self.feed_forward_img(x_img)\n",
        "\n",
        "                identity = x_dna\n",
        "                x_dna, _ = self.multi_head_dna_1(x_dna, x_img, x_img)\n",
        "                x_dna = self.norm_dna_1(x_dna + identity)\n",
        "                x_dna = self.feed_forward_dna(x_dna)\n",
        "\n",
        "                return x_img, x_dna\n",
        "\n",
        "        def feed_forward_img(self, x):\n",
        "                return self.linear_img_2(self.dropout_img(F.relu(self.linear_img_1(x))))\n",
        "\n",
        "        def feed_forward_dna(self, x):\n",
        "                return self.linear_dna_2(self.dropout_dna(F.relu(self.linear_dna_1(x))))\n",
        "\n",
        "class InceptionEncodersBlock(nn.Module):\n",
        "        def __init__(self, embed_dim_1, num_heads_1, embed_dim_2, num_heads_2, embed_dim_3, num_heads_3):\n",
        "                super(InceptionEncodersBlock, self).__init__()\n",
        "                self.img_dna_encoder_1 = ImageDNAEncoder(embed_dim_1, 4 * embed_dim_1, num_heads_1)\n",
        "                self.img_dna_encoder_2 = ImageDNAEncoder(embed_dim_2, 4 * embed_dim_2, num_heads_2)\n",
        "                self.img_dna_encoder_3 = ImageDNAEncoder(embed_dim_3, 4 * embed_dim_3, num_heads_3)\n",
        "\n",
        "                self.INPUT_SIZE = 500\n",
        "                self.embed_dim_1 = embed_dim_1\n",
        "                self.num_heads_1 = num_heads_1\n",
        "                self.embed_dim_2 = embed_dim_2\n",
        "                self.num_heads_2 = num_heads_2\n",
        "                self.embed_dim_3 = embed_dim_3\n",
        "                self.num_heads_3 = num_heads_3\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "\n",
        "                img_identity = torch.reshape(x_img.clone(), (-1, self.INPUT_SIZE ))\n",
        "                dna_identity = torch.reshape(x_dna.clone(), (-1, self.INPUT_SIZE ))\n",
        "                x_img_1 = torch.reshape(x_img.clone(), (-1, self.INPUT_SIZE // self.embed_dim_1, self.embed_dim_1))\n",
        "                x_dna_1 = torch.reshape(x_dna.clone(), (-1, self.INPUT_SIZE // self.embed_dim_1, self.embed_dim_1))\n",
        "                x_img_2 = torch.reshape(x_img.clone(), (-1, self.INPUT_SIZE // self.embed_dim_2, self.embed_dim_2))\n",
        "                x_dna_2 = torch.reshape(x_dna.clone(), (-1, self.INPUT_SIZE // self.embed_dim_2, self.embed_dim_2))\n",
        "                x_img_3 = torch.reshape(x_img.clone(), (-1, self.INPUT_SIZE // self.embed_dim_3, self.embed_dim_3))\n",
        "                x_dna_3 = torch.reshape(x_dna.clone(), (-1, self.INPUT_SIZE // self.embed_dim_3, self.embed_dim_3))\n",
        "\n",
        "                x_img_1, x_dna_1 = self.img_dna_encoder_1(x_img_1, x_dna_1)\n",
        "                x_img_2, x_dna_2 = self.img_dna_encoder_2(x_img_2, x_dna_2)\n",
        "                x_img_3, x_dna_3 = self.img_dna_encoder_3(x_img_3, x_dna_3)\n",
        "\n",
        "                x_img_1 = torch.reshape(x_img_1, (-1, self.INPUT_SIZE))\n",
        "                x_dna_1 = torch.reshape(x_dna_1, (-1, self.INPUT_SIZE))\n",
        "                x_img_2 = torch.reshape(x_img_2, (-1, self.INPUT_SIZE))\n",
        "                x_dna_2 = torch.reshape(x_dna_2, (-1, self.INPUT_SIZE))\n",
        "                x_img_3 = torch.reshape(x_img_3, (-1, self.INPUT_SIZE))\n",
        "                x_dna_3 = torch.reshape(x_dna_3, (-1, self.INPUT_SIZE))\n",
        "\n",
        "                x_img = img_identity + x_img_1 + x_img_2 + x_img_3\n",
        "                x_dna = dna_identity + x_dna_1 + x_dna_2 + x_dna_3\n",
        "\n",
        "                x_img = torch.concat((img_identity, x_img_1, x_img_2, x_img_3), axis=1)\n",
        "                x_dna = torch.concat((dna_identity, x_dna_1, x_dna_2, x_dna_3), axis=1)\n",
        "\n",
        "                return x_img, x_dna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEj9iqWHsaId"
      },
      "source": [
        "# Creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Of4Mq0hKsaId"
      },
      "outputs": [],
      "source": [
        "training_set = ImageDNATrainDataset()\n",
        "validation_set = ImageDNAValidationDataset()\n",
        "test_set = ImageDNATestDataset()\n",
        "training_validation_set = ImageDNATrainValidationDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93Uli0msaIe"
      },
      "source": [
        "Defining methods for training, validating and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ti1M7rRAusZp"
      },
      "outputs": [],
      "source": [
        "def validate(model, threshold, batch_size):\n",
        "\n",
        "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_predictions_per_labels = defaultdict(int)\n",
        "        total_samples_per_labels = defaultdict(int)\n",
        "        correct_predictions_per_genera = defaultdict(int)\n",
        "        total_samples_per_genera = defaultdict(int)\n",
        "\n",
        "        for data in validation_loader:\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            labels_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "\n",
        "            labels_outputs = nn.Softmax(dim=1)(labels_outputs)\n",
        "            genera_outputs = nn.Softmax(dim=1)(genera_outputs)\n",
        "\n",
        "            predicted_labels_values, predicted_labels = torch.topk(labels_outputs.data, k=2, dim=1)\n",
        "            _, predicted_genera = torch.max(genera_outputs.data, 1)\n",
        "\n",
        "            differences = predicted_labels_values[:, 0] - predicted_labels_values[:, 1]\n",
        "            genera_mask = differences <= threshold\n",
        "            labels_mask = ~genera_mask\n",
        "\n",
        "            # Update relative frequencies\n",
        "            for idx in range(len(genera)):\n",
        "                total_samples_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                if labels_mask[idx] and predicted_labels[idx, 0] == species[idx]:\n",
        "                    correct_predictions_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                # if the sample is of one undescribed species\n",
        "                if species[idx].item() not in np.unique(validation_set.seen_species):\n",
        "                    assert genera[idx].item() in np.unique(validation_set.unseen_species_genera)\n",
        "                    total_samples_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "                    if genera_mask[idx] and predicted_genera[idx] == genera[idx]:\n",
        "                        correct_predictions_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "        accuracy_per_label = {label: (correct_predictions_per_labels[label] / total_samples_per_labels[label]) if total_samples_per_labels[label] > 0 else 0 for label in total_samples_per_labels}\n",
        "        accuracy_per_genera = {genera: (correct_predictions_per_genera[genera] / total_samples_per_genera[genera]) if total_samples_per_genera[genera] > 0 else 0 for genera in total_samples_per_genera}\n",
        "\n",
        "        test_described_species_accuracy = 0\n",
        "        for label in np.unique(validation_set.seen_species):\n",
        "            test_described_species_accuracy += accuracy_per_label[label]\n",
        "\n",
        "        test_undescribed_species_accuracy = 0\n",
        "        for genera in np.unique(validation_set.unseen_species_genera):\n",
        "            test_undescribed_species_accuracy += accuracy_per_genera[genera]\n",
        "\n",
        "        normalized_test_described_species_accuracy = test_described_species_accuracy / 629\n",
        "        normalized_test_undescribed_species_accuracy = test_undescribed_species_accuracy / 97\n",
        "\n",
        "        return normalized_test_described_species_accuracy, normalized_test_undescribed_species_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "87_ldlBDusZp"
      },
      "outputs": [],
      "source": [
        "def test(model, threshold, batch_size):\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_predictions_per_labels = defaultdict(int)\n",
        "        total_samples_per_labels = defaultdict(int)\n",
        "        correct_predictions_per_genera = defaultdict(int)\n",
        "        total_samples_per_genera = defaultdict(int)\n",
        "\n",
        "        for data in test_loader:\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            labels_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "\n",
        "            labels_outputs = nn.Softmax(dim=1)(labels_outputs)\n",
        "            genera_outputs = nn.Softmax(dim=1)(genera_outputs)\n",
        "\n",
        "            predicted_labels_values, predicted_labels = torch.topk(labels_outputs.data, k=2, dim=1)\n",
        "            _, predicted_genera = torch.max(genera_outputs.data, 1)\n",
        "\n",
        "            differences = predicted_labels_values[:, 0] - predicted_labels_values[:, 1]\n",
        "            genera_mask = differences <= threshold\n",
        "            labels_mask = ~genera_mask\n",
        "\n",
        "            # Update relative frequencies\n",
        "            for idx in range(len(genera)):\n",
        "                total_samples_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                if labels_mask[idx] and predicted_labels[idx, 0] == species[idx]:\n",
        "                    correct_predictions_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                # if the sample is of one undescribed species\n",
        "                if species[idx].item() not in np.unique(test_set.seen_species):\n",
        "                    assert genera[idx].item() in np.unique(test_set.unseen_species_genera)\n",
        "                    total_samples_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "                    if genera_mask[idx] and predicted_genera[idx] == genera[idx]:\n",
        "                        correct_predictions_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "        accuracy_per_label = {label: (correct_predictions_per_labels[label] / total_samples_per_labels[label]) if total_samples_per_labels[label] > 0 else 0 for label in total_samples_per_labels}\n",
        "        accuracy_per_genera = {genera: (correct_predictions_per_genera[genera] / total_samples_per_genera[genera]) if total_samples_per_genera[genera] > 0 else 0 for genera in total_samples_per_genera}\n",
        "\n",
        "        test_described_species_accuracy = 0\n",
        "        for label in np.unique(test_set.seen_species):\n",
        "            test_described_species_accuracy += accuracy_per_label[label]\n",
        "\n",
        "        test_undescribed_species_accuracy = 0\n",
        "        for genera in np.unique(test_set.unseen_species_genera):\n",
        "            test_undescribed_species_accuracy += accuracy_per_genera[genera]\n",
        "\n",
        "        normalized_test_described_species_accuracy = test_described_species_accuracy / 770\n",
        "        normalized_test_undescribed_species_accuracy = test_undescribed_species_accuracy / 134\n",
        "\n",
        "        return normalized_test_described_species_accuracy, normalized_test_undescribed_species_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def train(model, lr, momentum, max_epochs, batch_size, train_val=False, print_losses=False, print_step=200):\n",
        "    model.train()\n",
        "    criterion_species = torch.nn.CrossEntropyLoss()\n",
        "    criterion_genera = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    \n",
        "    if train_val:\n",
        "        loader = torch.utils.data.DataLoader(training_validation_set, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
        "    \n",
        "    # Variables for early stopping\n",
        "    validation_genera_loss = 0\n",
        "    best_validation_genera_loss = np.inf\n",
        "    \n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        running_labels_loss = 0.0\n",
        "        running_genera_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            species_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "            labels_loss = criterion_species(species_outputs, species)\n",
        "            genera_loss = criterion_genera(genera_outputs, genera)\n",
        "            total_loss = labels_loss + genera_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print losses\n",
        "            if print_losses:\n",
        "                running_labels_loss += labels_loss.item()\n",
        "                running_genera_loss += genera_loss.item()\n",
        "                if i % print_step == print_step - 1:\n",
        "                    print(f\"[{epoch + 1}, {i + 1:5d}] Species loss: {running_labels_loss / print_step:.3f}; Genera loss: {running_genera_loss / print_step:.3f}\")\n",
        "                    running_labels_loss = 0.0\n",
        "                    running_genera_loss = 0.0\n",
        "\n",
        "        if (not train_val):\n",
        "            # Early stopping\n",
        "            patience = 10\n",
        "            validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True)\n",
        "            model.eval()\n",
        "            validation_genera_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for data in validation_loader:\n",
        "                    inputs_img, inputs_dna, species, genera = data\n",
        "                    inputs_img, inputs_dna, _, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "                    _, genera_outputs = model(inputs_img, inputs_dna)\n",
        "                    validation_genera_loss += criterion_genera(genera_outputs, genera).item() * inputs_img.size(0)\n",
        "\n",
        "            validation_genera_loss = validation_genera_loss / len(validation_loader.dataset)\n",
        "            print(f'Epoch {epoch+1}/{max_epochs} Validation genera Loss: {validation_genera_loss:.3f}')\n",
        "\n",
        "            # Check if validation loss improved\n",
        "            if validation_genera_loss < best_validation_genera_loss:\n",
        "                best_validation_genera_loss = validation_genera_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), 'best_model_early_stopping.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    if not train_val:\n",
        "        model.load_state_dict(torch.load('best_model_early_stopping.pth'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model selection\n",
        "We tune the following hyperparameters through model selection:\n",
        "- Learning rate\n",
        "- Threshold\n",
        "- Embedding dimensions and number of heads of the inception block\n",
        "We select the values that maximizes the sum of seen species accuracy and unseen genera accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zItvqJfusZq",
        "outputId": "86c6e8bb-9703-41ff-e6a1-f04172f9451a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 Validation genera Loss: 1.231\n",
            "Epoch 2/100 Validation genera Loss: 0.801\n",
            "Epoch 3/100 Validation genera Loss: 0.702\n",
            "Epoch 4/100 Validation genera Loss: 0.654\n",
            "Epoch 5/100 Validation genera Loss: 0.698\n",
            "Epoch 6/100 Validation genera Loss: 0.665\n",
            "Epoch 7/100 Validation genera Loss: 0.655\n",
            "Epoch 8/100 Validation genera Loss: 0.670\n",
            "Epoch 9/100 Validation genera Loss: 0.690\n",
            "Epoch 10/100 Validation genera Loss: 0.673\n",
            "Epoch 11/100 Validation genera Loss: 0.676\n",
            "Epoch 12/100 Validation genera Loss: 0.702\n",
            "Epoch 13/100 Validation genera Loss: 0.695\n",
            "Epoch 14/100 Validation genera Loss: 0.694\n",
            "Early stopping at epoch 14\n",
            "Validation accuracy: 1.574135584845338. Parameters: lr=0.01, threshold=0.7, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5769582329796583. Parameters: lr=0.01, threshold=0.7049152542372881, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5787460455003668. Parameters: lr=0.01, threshold=0.7098305084745762, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5791858423496818. Parameters: lr=0.01, threshold=0.7147457627118644, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.580630505666423. Parameters: lr=0.01, threshold=0.7196610169491525, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5820611381392298. Parameters: lr=0.01, threshold=0.7245762711864406, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5831105871485827. Parameters: lr=0.01, threshold=0.7294915254237287, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5841084571323945. Parameters: lr=0.01, threshold=0.7344067796610169, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5872373019603843. Parameters: lr=0.01, threshold=0.7393220338983051, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5914991315872076. Parameters: lr=0.01, threshold=0.7442372881355932, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5921038122157323. Parameters: lr=0.01, threshold=0.7491525423728813, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5945513165576837. Parameters: lr=0.01, threshold=0.7540677966101694, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5962409889426719. Parameters: lr=0.01, threshold=0.7589830508474575, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.597236910722259. Parameters: lr=0.01, threshold=0.7638983050847458, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5979637320355748. Parameters: lr=0.01, threshold=0.7688135593220339, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6006390813533553. Parameters: lr=0.01, threshold=0.773728813559322, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6034191868020775. Parameters: lr=0.01, threshold=0.7786440677966101, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6064091628363457. Parameters: lr=0.01, threshold=0.7835593220338983, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.609675187428021. Parameters: lr=0.01, threshold=0.7884745762711864, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6109548008671248. Parameters: lr=0.01, threshold=0.7933898305084746, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.612895366859442. Parameters: lr=0.01, threshold=0.7983050847457627, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6155128737844562. Parameters: lr=0.01, threshold=0.8032203389830508, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6198947405328714. Parameters: lr=0.01, threshold=0.8081355932203389, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.622630041762089. Parameters: lr=0.01, threshold=0.8130508474576271, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6235256943656253. Parameters: lr=0.01, threshold=0.8179661016949152, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.624783675438809. Parameters: lr=0.01, threshold=0.8228813559322034, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6242079633025783. Parameters: lr=0.01, threshold=0.8277966101694915, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6251132111031779. Parameters: lr=0.01, threshold=0.8327118644067797, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6257954946206448. Parameters: lr=0.01, threshold=0.8376271186440678, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.627017070950497. Parameters: lr=0.01, threshold=0.8425423728813559, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.629195788545409. Parameters: lr=0.01, threshold=0.847457627118644, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6325382745800612. Parameters: lr=0.01, threshold=0.8523728813559321, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6366554821982047. Parameters: lr=0.01, threshold=0.8572881355932203, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.639893202253759. Parameters: lr=0.01, threshold=0.8622033898305085, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6423582415445424. Parameters: lr=0.01, threshold=0.8671186440677966, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6425095912489813. Parameters: lr=0.01, threshold=0.8720338983050847, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6425842961645647. Parameters: lr=0.01, threshold=0.8769491525423728, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6435372231392509. Parameters: lr=0.01, threshold=0.881864406779661, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6438767892762702. Parameters: lr=0.01, threshold=0.8867796610169492, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6452117086524267. Parameters: lr=0.01, threshold=0.8916949152542373, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6465407108198815. Parameters: lr=0.01, threshold=0.8966101694915254, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.647730183719482. Parameters: lr=0.01, threshold=0.9015254237288135, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6488912095500043. Parameters: lr=0.01, threshold=0.9064406779661016, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6491055068526885. Parameters: lr=0.01, threshold=0.9113559322033898, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6486295777503852. Parameters: lr=0.01, threshold=0.916271186440678, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6470397526311484. Parameters: lr=0.01, threshold=0.9211864406779661, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6452880494422106. Parameters: lr=0.01, threshold=0.9261016949152542, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6457632437500305. Parameters: lr=0.01, threshold=0.9310169491525424, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6463440439669736. Parameters: lr=0.01, threshold=0.9359322033898305, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6447073253805886. Parameters: lr=0.01, threshold=0.9408474576271186, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6440328541178821. Parameters: lr=0.01, threshold=0.9457627118644067, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6431281288953885. Parameters: lr=0.01, threshold=0.9506779661016949, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6404246066952015. Parameters: lr=0.01, threshold=0.955593220338983, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.636821003091598. Parameters: lr=0.01, threshold=0.9605084745762712, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6317306671522624. Parameters: lr=0.01, threshold=0.9654237288135593, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6243085877047134. Parameters: lr=0.01, threshold=0.9703389830508474, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.6162216226316963. Parameters: lr=0.01, threshold=0.9752542372881357, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5976464009576266. Parameters: lr=0.01, threshold=0.9801694915254238, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.5765860175685593. Parameters: lr=0.01, threshold=0.9850847457627119, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Validation accuracy: 1.504657600567371. Parameters: lr=0.01, threshold=0.99, embed_dim_1=50, num_heads_1=5, embed_dim_2=100, num_heads_2=5, embed_dim_3=125, num_heads_3=5\n",
            "Best parameters: {'learning_rate': 0.01, 'threshold': 0.9113559322033898, 'embed_dim_1': 50, 'num_heads_1': 5, 'embed_dim_2': 100, 'num_heads_2': 5, 'embed_dim_3': 125, 'num_heads_3': 5}\n"
          ]
        }
      ],
      "source": [
        "lr_values = [0.01]\n",
        "threshold_values = np.linspace(0.7, 0.99, 60)\n",
        "model_parameters = [\n",
        "    #(10,  2, 20,  4, 25,   5),\n",
        "    #(20,  4, 25,  5, 50,   5),\n",
        "    #(25,  5, 50,  5, 100,  5),\n",
        "    (50,  5, 100, 5, 125,  5)\n",
        "    #(100, 5, 125, 5, 250, 10)\n",
        "]\n",
        "momentum = 0.9\n",
        "batch_size = 16\n",
        "max_epochs = 100\n",
        "\n",
        "best_validation_accuracy = 0\n",
        "best_parameters = {}\n",
        "\n",
        "for lr in lr_values:\n",
        "    for embed_dim_1, num_heads_1, embed_dim_2, num_heads_2, embed_dim_3, num_heads_3 in model_parameters:\n",
        "        model = AttentionNet(652, 368, embed_dim_1, num_heads_1, embed_dim_2, num_heads_2, embed_dim_3, num_heads_3)\n",
        "        model.to(device)\n",
        "        train(model, lr, momentum, max_epochs, batch_size)\n",
        "        for threshold in threshold_values:\n",
        "            validation_species_accuracy, validation_genera_accuracy = validate(model, threshold, batch_size)\n",
        "            validation_loss = validation_species_accuracy + validation_genera_accuracy\n",
        "            print((f\"Validation accuracy: {validation_loss}. \"\n",
        "                    f\"Parameters: lr={lr}, \"\n",
        "                    f\"threshold={threshold}, \"\n",
        "                    f\"embed_dim_1={embed_dim_1}, \"\n",
        "                    f\"num_heads_1={num_heads_1}, \"\n",
        "                    f\"embed_dim_2={embed_dim_2}, \"\n",
        "                    f\"num_heads_2={num_heads_2}, \"\n",
        "                    f\"embed_dim_3={embed_dim_3}, \"\n",
        "                    f\"num_heads_3={num_heads_3}\"))\n",
        "\n",
        "            if validation_loss > best_validation_accuracy:\n",
        "                best_validation_accuracy = validation_loss\n",
        "                best_parameters = {\n",
        "                    'learning_rate': lr,\n",
        "                    'threshold': threshold,\n",
        "                    'embed_dim_1': embed_dim_1,\n",
        "                    'num_heads_1': num_heads_1,\n",
        "                    'embed_dim_2': embed_dim_2,\n",
        "                    'num_heads_2': num_heads_2,\n",
        "                    'embed_dim_3': embed_dim_3,\n",
        "                    'num_heads_3': num_heads_3\n",
        "                }\n",
        "\n",
        "print(\"Best parameters:\", best_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final model training\n",
        "We select the best parameters found in the model selection section to train the final model on training and validation set.\n",
        "The model is then tested on the test set, which has $797$ seen species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJgoWU0AusZr",
        "outputId": "69ce2de9-42cf-4a2a-ae95-7aedb040c2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] Species loss: 5.159; Genera loss: 3.587\n",
            "[1,   400] Species loss: 3.192; Genera loss: 1.867\n",
            "[1,   600] Species loss: 2.130; Genera loss: 1.204\n",
            "[1,   800] Species loss: 1.496; Genera loss: 0.827\n",
            "[1,  1000] Species loss: 0.999; Genera loss: 0.585\n",
            "[1,  1200] Species loss: 0.612; Genera loss: 0.396\n",
            "[2,   200] Species loss: 0.271; Genera loss: 0.188\n",
            "[2,   400] Species loss: 0.176; Genera loss: 0.133\n",
            "[2,   600] Species loss: 0.107; Genera loss: 0.100\n",
            "[2,   800] Species loss: 0.067; Genera loss: 0.073\n",
            "[2,  1000] Species loss: 0.055; Genera loss: 0.049\n",
            "[2,  1200] Species loss: 0.047; Genera loss: 0.054\n",
            "[3,   200] Species loss: 0.028; Genera loss: 0.033\n",
            "[3,   400] Species loss: 0.029; Genera loss: 0.028\n",
            "[3,   600] Species loss: 0.017; Genera loss: 0.024\n",
            "[3,   800] Species loss: 0.019; Genera loss: 0.025\n",
            "[3,  1000] Species loss: 0.021; Genera loss: 0.023\n",
            "[3,  1200] Species loss: 0.012; Genera loss: 0.023\n",
            "[4,   200] Species loss: 0.009; Genera loss: 0.018\n",
            "[4,   400] Species loss: 0.010; Genera loss: 0.013\n",
            "[4,   600] Species loss: 0.013; Genera loss: 0.017\n",
            "[4,   800] Species loss: 0.012; Genera loss: 0.014\n",
            "[4,  1000] Species loss: 0.012; Genera loss: 0.012\n",
            "[4,  1200] Species loss: 0.008; Genera loss: 0.011\n",
            "[5,   200] Species loss: 0.005; Genera loss: 0.008\n",
            "[5,   400] Species loss: 0.015; Genera loss: 0.009\n",
            "[5,   600] Species loss: 0.005; Genera loss: 0.008\n",
            "[5,   800] Species loss: 0.007; Genera loss: 0.008\n",
            "[5,  1000] Species loss: 0.004; Genera loss: 0.009\n",
            "[5,  1200] Species loss: 0.010; Genera loss: 0.007\n",
            "[6,   200] Species loss: 0.003; Genera loss: 0.004\n",
            "[6,   400] Species loss: 0.004; Genera loss: 0.005\n",
            "[6,   600] Species loss: 0.004; Genera loss: 0.008\n",
            "[6,   800] Species loss: 0.004; Genera loss: 0.005\n",
            "[6,  1000] Species loss: 0.004; Genera loss: 0.006\n",
            "[6,  1200] Species loss: 0.003; Genera loss: 0.005\n",
            "[7,   200] Species loss: 0.003; Genera loss: 0.004\n",
            "[7,   400] Species loss: 0.004; Genera loss: 0.006\n",
            "[7,   600] Species loss: 0.003; Genera loss: 0.004\n",
            "[7,   800] Species loss: 0.002; Genera loss: 0.004\n",
            "[7,  1000] Species loss: 0.003; Genera loss: 0.007\n",
            "[7,  1200] Species loss: 0.003; Genera loss: 0.010\n",
            "[8,   200] Species loss: 0.003; Genera loss: 0.003\n",
            "[8,   400] Species loss: 0.002; Genera loss: 0.003\n",
            "[8,   600] Species loss: 0.003; Genera loss: 0.004\n",
            "[8,   800] Species loss: 0.003; Genera loss: 0.003\n",
            "[8,  1000] Species loss: 0.002; Genera loss: 0.003\n",
            "[8,  1200] Species loss: 0.002; Genera loss: 0.004\n",
            "[9,   200] Species loss: 0.001; Genera loss: 0.003\n",
            "[9,   400] Species loss: 0.002; Genera loss: 0.003\n",
            "[9,   600] Species loss: 0.001; Genera loss: 0.003\n",
            "[9,   800] Species loss: 0.003; Genera loss: 0.003\n",
            "[9,  1000] Species loss: 0.002; Genera loss: 0.005\n",
            "[9,  1200] Species loss: 0.001; Genera loss: 0.003\n",
            "[10,   200] Species loss: 0.001; Genera loss: 0.002\n",
            "[10,   400] Species loss: 0.002; Genera loss: 0.004\n",
            "[10,   600] Species loss: 0.002; Genera loss: 0.003\n",
            "[10,   800] Species loss: 0.001; Genera loss: 0.002\n",
            "[10,  1000] Species loss: 0.001; Genera loss: 0.003\n",
            "[10,  1200] Species loss: 0.001; Genera loss: 0.002\n",
            "-------------------------------------------------------------------------------\n",
            "Final model described species accuracy:  0.988119227179433\n",
            "Final model undescribed species accuracy:  0.6972008363717906\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = AttentionNet(797, 368, best_parameters['embed_dim_1'], best_parameters['num_heads_1'], best_parameters['embed_dim_2'], best_parameters['num_heads_2'], best_parameters['embed_dim_3'], best_parameters['num_heads_3'])\n",
        "model.to(device)\n",
        "train(model, best_parameters['learning_rate'], momentum, 10, batch_size, train_val=True, print_losses=True)\n",
        "species_accuracy, genera_accuracy = test(model, best_parameters['threshold'], batch_size)\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(f\"Final model described species accuracy: \", species_accuracy)\n",
        "print(f\"Final model undescribed species accuracy: \", genera_accuracy)\n",
        "print(\"-------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
